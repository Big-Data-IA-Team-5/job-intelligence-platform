"""
Generate embeddings for jobs missing them
Bypasses dbt and directly creates embeddings in Snowflake
"""
import json
import snowflake.connector
from pathlib import Path
import sys

def generate_embeddings():
    """Generate embeddings for all jobs in JOBS_RAW that don't have them yet"""
    
    # Load secrets - check multiple possible locations
    possible_paths = [
        Path('/opt/airflow/secrets/secrets.json'),  # Docker mount location
        Path(__file__).parent.parent / 'secrets.json',  # Local development
    ]
    
    secrets_path = None
    for path in possible_paths:
        if path.exists():
            secrets_path = path
            break
    
    if not secrets_path:
        raise FileNotFoundError(f"Could not find secrets.json in any of: {possible_paths}")
    
    with open(secrets_path, 'r') as f:
        secrets = json.load(f)
    
    sf = secrets['snowflake']
    
    # Connect to Snowflake
    print("üîå Connecting to Snowflake...")
    conn = snowflake.connector.connect(
        account=sf['account'],
        user=sf['user'],
        password=sf['password'],
        database='JOB_INTELLIGENCE',
        warehouse=sf['warehouse'],
        schema='PROCESSED_PROCESSING',
        ocsp_fail_open=True,  # Allow connection even if OCSP check fails
        insecure_mode=False   # Keep SSL enabled but be more lenient
    )
    
    cursor = conn.cursor()
    
    try:
        # Step 1: Check current state
        print("\nüìä Checking current state...")
        cursor.execute("SELECT COUNT(*) FROM RAW.JOBS_RAW")
        jobs_raw_count = cursor.fetchone()[0]
        print(f"   ‚Ä¢ JOBS_RAW: {jobs_raw_count:,} jobs")
        
        cursor.execute("SELECT COUNT(*) FROM PROCESSED.JOBS_PROCESSED")
        processed_count = cursor.fetchone()[0]
        print(f"   ‚Ä¢ JOBS_PROCESSED: {processed_count:,} jobs")
        
        missing = jobs_raw_count - processed_count
        print(f"   ‚Ä¢ Missing from JOBS_PROCESSED: {missing:,} jobs\n")
        
        if missing <= 0:
            print("‚úÖ All jobs already in JOBS_PROCESSED!")
            return None
        
        # Step 2: Get jobs that need to be added to JOBS_PROCESSED
        # Use EMBEDDED_JOBS (dbt deduplicated) as source of truth
        print("üîç Finding jobs missing from JOBS_PROCESSED...")
        cursor.execute("""
            SELECT DISTINCT e.job_id
            FROM PROCESSED_PROCESSING.EMBEDDED_JOBS e
            LEFT JOIN PROCESSED.JOBS_PROCESSED p ON e.job_id = p.job_id
            WHERE p.job_id IS NULL
               OR e.scraped_at > p.scraped_at  -- Update if source is newer
            LIMIT 5000
        """)
        missing_job_ids = [row[0] for row in cursor.fetchall()]
        print(f"   Found {len(missing_job_ids):,} jobs to process (new or updated)")
        
        if not missing_job_ids:
            print("‚úÖ No missing jobs!")
            return None
        
        # Step 3: Sync JOBS_PROCESSED with existing embeddings
        print(f"\nüöÄ Syncing JOBS_PROCESSED with embedded jobs...")
        batch_size = 100
        total_processed = 0
        
        for i in range(0, len(missing_job_ids), batch_size):
            batch = missing_job_ids[i:i+batch_size]
            batch_ids = "', '".join(batch)
            
            sql = f"""
            MERGE INTO PROCESSED.JOBS_PROCESSED p
            USING (
                SELECT
                    e.job_id,
                    r.source,
                    r.url,
                    e.title,
                    e.company,
                    e.location,
                    e.description,
                    r.snippet,
                    r.job_type,
                    r.posted_date,
                    e.scraped_at,
                    r.salary_min,
                    r.salary_max,
                    r.salary_text,
                    e.h1b_sponsor,
                    e.h1b_employer_name,
                    e.h1b_city,
                    e.h1b_state,
                    e.total_petitions,
                    e.avg_approval_rate,
                    e.description_embedding,
                    r.processed_at,
                    e.company as company_clean,
                    'H-1B' as visa_category,
                    '' as qualifications,
                    DATEDIFF(day, r.posted_date, CURRENT_DATE()) as days_since_posted,
                    1.0 as classification_confidence,
                    CASE WHEN e.is_remote THEN 'Remote' ELSE 'On-site' END as work_model,
                    '' as department,
                    '' as company_size,
                    FALSE as h1b_sponsored_explicit,
                    FALSE as is_new_grad_role,
                    COALESCE(e.job_category, '') as job_category,
                    e.avg_approval_rate as h1b_approval_rate,
                    e.total_petitions as h1b_total_petitions,
                    NULL as sponsorship_score,
                    NULL as h1b_risk_level,
                    NULL as h1b_avg_wage
                FROM PROCESSED_PROCESSING.EMBEDDED_JOBS e
                JOIN RAW.JOBS_RAW r ON e.job_id = r.job_id AND e.source = r.source
                WHERE e.job_id IN ('{batch_ids}')
            ) e
            ON p.job_id = e.job_id
            WHEN MATCHED AND e.scraped_at > p.scraped_at THEN UPDATE SET
                title = e.title,
                company = e.company,
                location = e.location,
                description = e.description,
                scraped_at = e.scraped_at,
                description_embedding = e.description_embedding,
                processed_at = e.processed_at
            WHEN NOT MATCHED THEN INSERT (
                JOB_ID, SOURCE, URL, TITLE, COMPANY, LOCATION, DESCRIPTION, 
                SNIPPET, JOB_TYPE, POSTED_DATE, SCRAPED_AT,
                SALARY_MIN, SALARY_MAX, SALARY_TEXT,
                H1B_SPONSOR, H1B_EMPLOYER_NAME, H1B_CITY, H1B_STATE,
                DESCRIPTION_EMBEDDING, PROCESSED_AT,
                COMPANY_CLEAN, VISA_CATEGORY, QUALIFICATIONS,
                DAYS_SINCE_POSTED, CLASSIFICATION_CONFIDENCE,
                WORK_MODEL, DEPARTMENT, COMPANY_SIZE,
                H1B_SPONSORED_EXPLICIT, IS_NEW_GRAD_ROLE, JOB_CATEGORY,
                H1B_APPROVAL_RATE, H1B_TOTAL_PETITIONS,
                SPONSORSHIP_SCORE, H1B_RISK_LEVEL, H1B_AVG_WAGE
            )
            VALUES (
                e.job_id, e.source, e.url, e.title, e.company, e.location, e.description,
                e.snippet, e.job_type, e.posted_date, e.scraped_at,
                e.salary_min, e.salary_max, e.salary_text,
                e.h1b_sponsor, e.h1b_employer_name, e.h1b_city, e.h1b_state,
                e.description_embedding, e.processed_at,
                e.company_clean, e.visa_category, e.qualifications,
                e.days_since_posted, e.classification_confidence,
                e.work_model, e.department, e.company_size,
                e.h1b_sponsored_explicit, e.is_new_grad_role, e.job_category,
                e.h1b_approval_rate, e.h1b_total_petitions,
                e.sponsorship_score, e.h1b_risk_level, e.h1b_avg_wage
            )
            """
            
            cursor.execute(sql)
            total_processed += len(batch)
            print(f"   ‚úì Synced {total_processed:,}/{len(missing_job_ids):,} jobs")
        
        # Step 4: Verify sync
        print("\nüîç Verifying sync...")
        
        # Step 5: Final counts
        cursor.execute("SELECT COUNT(*) FROM PROCESSED.JOBS_PROCESSED")
        final_count = cursor.fetchone()[0]
        
        print(f"\n‚úÖ Success!")
        print(f"   ‚Ä¢ New jobs processed: {total_processed:,}")
        print(f"   ‚Ä¢ Total searchable jobs: {final_count:,}")
        
        return total_processed
        
    except Exception as e:
        print(f"\n‚ùå Error: {e}")
        raise
    finally:
        cursor.close()
        conn.close()
        print("\nüîå Connection closed")

def main():
    """Main function that loops until all jobs are processed"""
    print("üöÄ Starting embedding generation (will process all pending jobs)")
    print("=" * 80)
    
    iteration = 0
    total_processed = 0
    
    while True:
        iteration += 1
        print(f"\nüìç Iteration {iteration}")
        print("-" * 80)
        
        try:
            result = generate_embeddings()
            
            # Check if we're done
            if result is None:  # No more jobs to process
                break
                
            total_processed += 5000  # Each iteration processes up to 5000
            
        except Exception as e:
            print(f"\n‚ùå Error in iteration {iteration}: {e}")
            break
    
    print("\n" + "=" * 80)
    print("üéâ ALL JOBS PROCESSED!")
    print("=" * 80)
    print(f"   ‚Ä¢ Total iterations: {iteration}")
    print(f"   ‚Ä¢ Approximate jobs processed: {total_processed:,}")
    
    return {"iterations": iteration, "total_processed": total_processed}

if __name__ == "__main__":
    main()
